{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########import packages##########\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.constraints import max_norm\n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense \n",
    "from keras.layers import Dropout \n",
    "from keras.layers import BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasClassifier \n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#%matplotlib\n",
    "###########fix random seed for reproducability##########\n",
    "seed=1\n",
    "np.random.seed(seed)\n",
    "file = open('parameter.txt','w')\n",
    "###########wrapping root mean square error for later calls##########\n",
    "def rmse(predictions, targets):\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "###########loading data##########\n",
    "csv = \"../../database/database37.csv\"\n",
    "\n",
    "#fdata=pd.read_csv(csv, index_col=0)\n",
    "fdata=pd.read_csv(csv, encoding = 'gbk', index_col=0)\n",
    "print(fdata)\n",
    "raw_data = fdata.iloc[:, 1:]\n",
    "\n",
    "median_raw_data=raw_data.median()\n",
    "dict_median_raw_data=median_raw_data.to_dict()\n",
    "data=raw_data.fillna(dict_median_raw_data)\n",
    "#print(data)\n",
    "#print(np.std(data,axis=0))\n",
    "###########data standardization##########\n",
    "standardized_data = (data-np.mean(data,axis=0))/np.std(data,axis=0)\n",
    "#standardized_data = (data-data.mean(axis=0))/data.std(axis=0)\n",
    "raw_param=standardized_data.iloc[:,0:36]\n",
    "raw_power=standardized_data.iloc[:,36]\n",
    "#print(raw_param)\n",
    "print(raw_power)\n",
    "X=raw_param.values.astype(np.float32)\n",
    "y=raw_power.values.astype(np.float32)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.15, random_state=751)\n",
    "###########search neuron network hyperparmeter space##########\n",
    "dropout_list= np.arange(0, 1, 0.1)\n",
    "neurons=240\n",
    "for activation in ['relu','tanh','softsign']:\n",
    "    for regularizer_term in [0.0001,0.0002,0.0005,0.0008,0.001,0.002,0.005,0.0008,0.01,0.02,0.05,0.08,0.1]:\n",
    "        for dropout in dropout_list:\n",
    "            for epochs_number in range(200,400,600,800,1000,1200,1400,1600,1800):\n",
    "                for batch_size_number in range (20,2000,40):\n",
    "                    for learning_rate_search in [0.001,0.002,0.005,0.01]:\n",
    "                        ###########implementing hyperparameters##########\n",
    "                        neurons1=neurons\n",
    "                        activation1=activation\n",
    "                        regularizer=keras.regularizers.l2(regularizer_term)\n",
    "                        dropout_rate=dropout\n",
    "                        ###########keras ANN model construction##########\n",
    "                        model = Sequential() \n",
    "                        model.add(Dense(neurons1, input_dim=36, kernel_initializer='random_normal',\n",
    "                                        bias_initializer='random_normal',activation=activation1,kernel_regularizer=regularizer)) \n",
    "                        model.add(Dropout(dropout_rate))\n",
    "                        model.add(Dense(neurons1, input_dim=neurons1, kernel_initializer='random_normal',\n",
    "                                    bias_initializer='random_normal',activation=activation1,kernel_regularizer=regularizer)) \n",
    "                        model.add(Dropout(dropout_rate)) \n",
    "                        model.add(Dense(neurons1, input_dim=neurons1, kernel_initializer='random_normal',\n",
    "                                    bias_initializer='random_normal',activation=activation1,kernel_regularizer=regularizer)) \n",
    "                        model.add(Dropout(dropout_rate))\n",
    "                        model.add(Dense(neurons1, input_dim=neurons1, kernel_initializer='random_normal',\n",
    "                                    bias_initializer='random_normal',activation=activation1,kernel_regularizer=regularizer)) \n",
    "                        model.add(Dropout(dropout_rate)) \n",
    "                        model.add(Dense(1, input_dim=neurons1, activation='linear'))\n",
    "                        adam=tf.keras.optimizers.Adam(lr=learning_rate_search)\n",
    "                        model.compile(loss='mse', optimizer=adam) \n",
    "                        #print('Training ------------')\n",
    "                        ###########train the model with the training set##########\n",
    "                        ###########testset has been remove before##########\n",
    "                        model.fit(X_train, y_train,verbose=0, epochs=epochs_number, batch_size=batch_size_number,validation_split=0.1,callbacks=[TensorBoard(log_dir='mytensorboard')])\n",
    "                        loss= model.evaluate(X_test, y_test)\n",
    "                        predict_ann= model.predict(X_test)\n",
    "                        train_ann= model.predict(X_train)\n",
    "                        ###########result output##########\n",
    "                        x_prediction_maximum_power_ann=predict_ann*np.std(data,axis=0)[36]+np.mean(data,axis=0)[36]\n",
    "                        y_real_maximum_power=y_test*np.std(data,axis=0)[36]+np.mean(data,axis=0)[36]\n",
    "                        x_prediction_maximum_power_ann=x_prediction_maximum_power_ann[:,0]\n",
    "                        x_prediction_maximum_power_ann_series=pd.Series(x_prediction_maximum_power_ann)\n",
    "                        y_real_maximum_power_series=pd.Series(y_real_maximum_power)\n",
    "                        corr_ann = round(x_prediction_maximum_power_ann_series.corr(y_real_maximum_power_series), 4)\n",
    "                        rmse_val= rmse(x_prediction_maximum_power_ann,y_real_maximum_power)\n",
    "                        print('ANN,R2',corr_ann,'RMSE',rmse_val)\n",
    "                        if corr_ann>0.7:\n",
    "                            file.write('ANN,R2'+' '+ 'RMSE'+' '+ '\\n')\n",
    "                            mm1 ='%f'%corr_ann\n",
    "                            mw1 ='%f'%rmse_val\n",
    "                            neurons1 = '%d'%neurons\n",
    "                            regularizer_term1='%f'%regularizer_term\n",
    "                            dropout1='%f'%dropout\n",
    "                            epochs_number1='%f'%epochs_number\n",
    "                            batch_size_number1='%f'%batch_size_number\n",
    "                            file.write('ANN,R2'+' '+ mm1 +' '+ 'RMSE'+' '+ mw1 +' '+neurons1+' '+regularizer_term1+' '+dropout1+' '+ epochs_number1+' '+batch_size_number1+'\\n')\n",
    "                            if corr_ann > max_corr_ann:\n",
    "                                max_corr_ann = corr_ann\n",
    "                                max_rmse_val = rmse_val\n",
    "                                max_y_real_maximum_power = y_real_maximum_power\n",
    "                                max_x_prediction_maximum_power_ann = x_prediction_maximum_power_ann\n",
    "                                max_learning_rate_search = learning_rate_search\n",
    "                                max_neurons = neurons\n",
    "                                max_activation = activation\n",
    "                                max_regularizer_term = regularizer_term\n",
    "                                max_dropout = dropout\n",
    "                                max_epochs_number = epochs_number\n",
    "                                max_batch_size_number = batch_size_number\n",
    "                        if 0.9<corr_ann<1:\n",
    "                            break\n",
    "                        else:\n",
    "                            K.clear_session()\n",
    "                    else:continue\n",
    "                    break\n",
    "                else:continue\n",
    "                break\n",
    "            else:continue\n",
    "            break\n",
    "        else:continue\n",
    "        break\n",
    "    else:continue\n",
    "    break\n",
    "\n",
    "###########print best hyperparameter##########\n",
    "print(learning_rate_search)\n",
    "print(neurons)\n",
    "print(activation)\n",
    "print(regularizer_term)\n",
    "print(dropout)\n",
    "print(epochs_number)\n",
    "print(batch_size_number)\n",
    "print('ANN,R2',corr_ann)\n",
    "print('ANN,RMSE',rmse_val)\n",
    "print(y_real_maximum_power)\n",
    "###########visualization##########"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
